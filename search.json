[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "K-Means Clustering & Bofedales",
    "section": "",
    "text": "Introduction",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#what-are-bofedales",
    "href": "index.html#what-are-bofedales",
    "title": "K-Means Clustering & Bofedales",
    "section": "What are bofedales?",
    "text": "What are bofedales?\nBofedales are unique high-Andean wetland ecosystems characterized by peat formation. Located over 3,000m above sea level, they originate due to permanent water flows that foster the development of cushion plants. Cushion-forming Juncaceae are “nurse plants” for plants consumed by wild and domesticated camelids. Bofedales are essential for water management, have high cultural value for indigenous communities (especially herders and farmers), and play a key role in the global climate systems. They can be found in Peru, Bolivia, and northern Chile, the region focused on by this study.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#what-is-a-k-means-clustering-algorithm",
    "href": "index.html#what-is-a-k-means-clustering-algorithm",
    "title": "K-Means Clustering & Bofedales",
    "section": "What is a k-means clustering algorithm?",
    "text": "What is a k-means clustering algorithm?\nK-means clustering is a type of unsupervised machine learning model. If all of the n features of the dataset were graphed in a n-dimensional space, k-means attempts to create a set of centers and minimize the sum of squared distances between any point and its center. Since this problem is NP-hard, k-means is a heuristic algorithm that converges as a local optimum.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "research-qs.html",
    "href": "research-qs.html",
    "title": "1  Research Questions & Objectives",
    "section": "",
    "text": "1.1 Research Question",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Research Questions & Objectives</span>"
    ]
  },
  {
    "objectID": "research-qs.html#research-question",
    "href": "research-qs.html#research-question",
    "title": "1  Research Questions & Objectives",
    "section": "",
    "text": "1.1.1 Main Question\nWhat can a machine learning algorithm reveal about the impact of various factors on the health of bofedales in Chile?\n\n\n1.1.2 Sub-Questions\n\nWhat models are effective for analyzing the impact of various factors on the health of bofedales in Chile?\nWhat factors prove most influential to the health of bofedales in Chile based on available data?\nWhat can the clusters produced by the model reveal about Chilean bofedal health?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Research Questions & Objectives</span>"
    ]
  },
  {
    "objectID": "research-qs.html#research-objectives",
    "href": "research-qs.html#research-objectives",
    "title": "1  Research Questions & Objectives",
    "section": "1.2 Research Objectives",
    "text": "1.2 Research Objectives\n\n1.2.1 Main Objective\nBuild and train several different machine learning algorithms on various factors that may affect Chilean bofedal health.\n\n\n1.2.2 Sub-Objectives\n\nDetermine which types of ML algorithms best reveal impacts to bofedal health.\nInterpret the weights given to each variable by the ML model.\nAnalyze clusters to determine the meaning of each cluster and its typical attributes.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Research Questions & Objectives</span>"
    ]
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "2  Datasets & Variables",
    "section": "",
    "text": "2.1 The CAMELS-CL dataset\nThese datasets have temporal data, beginning around 1980 and ending around 2020 (depending on the dataset)\nLink to the CAMELS-CL dataset: https://doi.pangaea.de/10.1594/PANGAEA.894885\nNote: The CAMELS-CL dataset does include streamflow data, but due to large amounts of missing data, it was not incorporated into the model",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Datasets & Variables</span>"
    ]
  },
  {
    "objectID": "datasets.html#the-camels-cl-dataset",
    "href": "datasets.html#the-camels-cl-dataset",
    "title": "2  Datasets & Variables",
    "section": "",
    "text": "Minimum Temperature\nMaximum Temperature\nPrecipitation\nGround Water Rights\nSurface Water Rights\nPET",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Datasets & Variables</span>"
    ]
  },
  {
    "objectID": "datasets.html#extracted-from-sentinel-2-satellite-imagery-from-gee",
    "href": "datasets.html#extracted-from-sentinel-2-satellite-imagery-from-gee",
    "title": "2  Datasets & Variables",
    "section": "2.2 Extracted from Sentinel-2 Satellite Imagery from GEE",
    "text": "2.2 Extracted from Sentinel-2 Satellite Imagery from GEE\n\nNDVI\nNDWI\n\nLink to the Sentinel-2 dataset: https://dataspace.copernicus.eu/explore-data/data-collections/sentinel-data/sentinel-2\nThese datasets have temporal data from 2019 to 2024",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Datasets & Variables</span>"
    ]
  },
  {
    "objectID": "datasets.html#opentopographys-api-service",
    "href": "datasets.html#opentopographys-api-service",
    "title": "2  Datasets & Variables",
    "section": "2.3 OpenTopography’s API service",
    "text": "2.3 OpenTopography’s API service\n\nElevation\nElevation standard deviation\n\nLink to OpenTopography’s website: https://opentopography.org/",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Datasets & Variables</span>"
    ]
  },
  {
    "objectID": "datasets.html#dga-datasets",
    "href": "datasets.html#dga-datasets",
    "title": "2  Datasets & Variables",
    "section": "2.4 DGA datasets",
    "text": "2.4 DGA datasets\n\nNumber of reservoirs\n\nLink to download: https://dga.mop.gob.cl/uploads/sites/13/2024/07/Embalses-1.zip",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Datasets & Variables</span>"
    ]
  },
  {
    "objectID": "datasets.html#development-of-groundwater-levels-dataset-for-chile-since-1970-publication-in-nature",
    "href": "datasets.html#development-of-groundwater-levels-dataset-for-chile-since-1970-publication-in-nature",
    "title": "2  Datasets & Variables",
    "section": "2.5 Development of Groundwater Levels Dataset for Chile since 1970 publication in Nature",
    "text": "2.5 Development of Groundwater Levels Dataset for Chile since 1970 publication in Nature\n\nNumber of boreholes/wells\n\nLink to the publication: https://www.nature.com/articles/s41597-023-02895-5",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Datasets & Variables</span>"
    ]
  },
  {
    "objectID": "datasets.html#mapbiomas",
    "href": "datasets.html#mapbiomas",
    "title": "2  Datasets & Variables",
    "section": "2.6 MapBiomas",
    "text": "2.6 MapBiomas\n\nProtected/park land data (m^2 and percentage of whole)\n\nLink to MapBiomas: https://chile.mapbiomas.org/en/mapas-de-la-coleccion/",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Datasets & Variables</span>"
    ]
  },
  {
    "objectID": "datasets.html#variables-not-included",
    "href": "datasets.html#variables-not-included",
    "title": "2  Datasets & Variables",
    "section": "2.7 Variables not included",
    "text": "2.7 Variables not included\nSeveral variables that are important for consideration when examining factors that determine overall health of a bofedal but do not have readily avalible datasets include:\n\nBofedal management strategies\nPopulation dynamics in herding communities\nCarrying capacity of ganandos by species\n\nSocioeconomic variables such as these are addressed later in the study through case studies and field work.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Datasets & Variables</span>"
    ]
  },
  {
    "objectID": "Notebooks/PCA.html",
    "href": "Notebooks/PCA.html",
    "title": "3  Principle Component Analysis",
    "section": "",
    "text": "3.1 Import Statements\nSince the merge between the datasets created a lot of columns (over 2,000), it’s necesary to reduce the dimensions of the dataset with PCA before running the k-means algorithm.\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndf = pd.read_csv(\"bofedales-clean.csv\")\ndf = df.drop([\"Unnamed: 0\"], axis=1)\ndf.isna().sum().sum()\n\nnp.int64(0)\ndf.shape\n\n(2534, 2289)\ndf.head(3)\n\n\n\n\n\n\n\n\nArea_m2\nAUC\npct_prot\nelev_mean_\nelev_std_m\nn_wells\nGround Water Rights 1966-01-01\nGround Water Rights 1967-01-01\nGround Water Rights 1968-01-01\nGround Water Rights 1969-01-01\n...\nNDWI 2019-03\nNDWI 2019-04\nNDWI 2019-05\nNDWI 2019-06\nNDWI 2019-07\nNDWI 2019-08\nNDWI 2019-09\nNDWI 2019-10\nNDWI 2019-11\nNDWI 2019-12\n\n\n\n\n0\n6300\n86.769539\n0.0\n4162.714286\n3.953815\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.031930\n0.026136\n0.022087\n0.019181\n0.023405\n0.015355\n-0.000504\n0.004056\n0.014678\n0.010436\n\n\n1\n5400\n83.176353\n0.0\n4073.500000\n12.406316\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n-0.057992\n-0.053230\n-0.054671\n-0.064990\n-0.063351\n-0.074670\n-0.085071\n-0.081491\n-0.061333\n-0.055450\n\n\n2\n6300\n103.719438\n0.0\n4278.571429\n6.161102\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.065899\n0.070822\n0.070075\n0.068015\n0.068144\n0.062340\n0.047670\n0.050570\n0.056227\n0.059294\n\n\n\n\n3 rows × 2289 columns",
    "crumbs": [
      "Running K-Means",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Principle Component Analysis</span>"
    ]
  },
  {
    "objectID": "Notebooks/PCA.html#finding-the-optimal-number-of-principle-components",
    "href": "Notebooks/PCA.html#finding-the-optimal-number-of-principle-components",
    "title": "3  Principle Component Analysis",
    "section": "3.2 Finding the Optimal Number of Principle Components",
    "text": "3.2 Finding the Optimal Number of Principle Components\nFirst, check how many principle components account for 90% of the variance in the data\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(df)  \n\n\nx_per_variance = 0.90\n\npca = PCA(n_components=x_per_variance, svd_solver=\"full\")\nX_pca = pca.fit_transform(X_scaled)\nprint(f\"Number of principal components chosen to explain {x_per_variance * 100}% variance: {pca.n_components_}\")\nprint(\"Explained variance ratio (per PC):\")\nprint(np.round(pca.explained_variance_ratio_, 4))\nprint(\"Cumulative explained variance:\", round(pca.explained_variance_ratio_.cumsum()[-1], 4))\n\nNumber of principal components chosen to explain 90.0% variance: 5\nExplained variance ratio (per PC):\n[0.7058 0.0929 0.0571 0.0397 0.0334]\nCumulative explained variance: 0.9289\n\n\nFive PCs account for 90% of the variance, so the data can easily be greatly reduced in dimensionality.\nNext, graph variance versus PCs. The elbow in the graph has the optional number of PCs – afterwards, there are diminishing returns.\n\nfeatures_for_pca = df.columns.tolist()\n\npca_full = PCA()\npca_full.fit(X_scaled)\n\ncum_var = np.cumsum(pca_full.explained_variance_ratio_)\n\n# 6) Plot the \"elbow\" (cumulative explained variance vs. number of components)\nplt.figure(figsize=(8, 5))\nplt.plot(range(1, len(cum_var) + 1), cum_var, marker='o', linewidth=2)\nplt.axhline(y=0.90, color='r', linestyle='--', label='90% Threshold')\nplt.axhline(y=0.99, color='g', linestyle='--', label='99% Threshold')\nplt.xlabel(\"Number of Principal Components\")\nplt.ylabel(\"Cumulative Explained Variance\")\nplt.title(\"Elbow Plot: Cumulative Explained Variance by Number of PCs\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\n\nplt.xlim(1, 50)\n\nplt.show()\n\n\n\n\n\n\n\n\nSince the “elbow” of the curve occurs at around 7 or 6 PCs, this is the value that will be used in analyzing the data",
    "crumbs": [
      "Running K-Means",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Principle Component Analysis</span>"
    ]
  },
  {
    "objectID": "Notebooks/PCA.html#run-pca-with-6-components",
    "href": "Notebooks/PCA.html#run-pca-with-6-components",
    "title": "3  Principle Component Analysis",
    "section": "3.3 Run PCA with 6 Components",
    "text": "3.3 Run PCA with 6 Components\n\npca6 = PCA(n_components=6)\nX_ = pca6.fit_transform(X_scaled)\n\nSave the dataset to be run in future notebooks:\n\nnp.save(\"scaled-df.npy\", X_)",
    "crumbs": [
      "Running K-Means",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Principle Component Analysis</span>"
    ]
  },
  {
    "objectID": "Notebooks/k7.html",
    "href": "Notebooks/k7.html",
    "title": "4  Running k=7",
    "section": "",
    "text": "4.1 Import Statements\nSince the merge between the datasets created a lot of columns (over 2,000), it’s necesary to reduce the dimensions of the dataset with PCA before running the k-means algorithm.\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score, pairwise_distances_argmin_min, silhouette_samples\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\n\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport contextily as ctx\nimport folium\n\nfrom matplotlib.lines import Line2D\nimport matplotlib.dates as mdates\nimport matplotlib.lines as mlines\nimport matplotlib.colors as mcolors\nimport matplotlib.cm as cm\n\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interactive\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nX_ = np.load(\"scaled-df.npy\")\ndf = pd.read_csv(\"bofedales-clean.csv\")\nThe K-Means algorithm will run on the dataset in PC space.",
    "crumbs": [
      "Running K-Means",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Running k=7</span>"
    ]
  },
  {
    "objectID": "Notebooks/k7.html#find-the-optional-amount-of-groups-for-k-means",
    "href": "Notebooks/k7.html#find-the-optional-amount-of-groups-for-k-means",
    "title": "4  Running k=7",
    "section": "4.2 Find the Optional Amount of Groups for K-Means",
    "text": "4.2 Find the Optional Amount of Groups for K-Means\n\ninertias = []\nK_candidates = list(range(2, 15))\n\nfor k in K_candidates:\n    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n    km.fit(X_)\n    inertias.append(km.inertia_)\n\nplt.figure(figsize=(6, 4))\nplt.plot(K_candidates, inertias, marker='o', lw=2)\nplt.xlabel(\"Number of clusters K\")\nplt.ylabel(\"KMeans inertia (sum of squared distances)\")\nplt.title(\"Elbow Method on 6-PC data\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFrom the graph, it looks like around 5-7 is the optional k value. However, k=2 is also an optimal value.\n\ndef kmeans_metrics(X, k_min=2, k_max=15, random_state=42):\n    rows = []\n    for k in range(k_min, k_max + 1):\n        km = KMeans(n_clusters=k, random_state=random_state, n_init=\"auto\")\n        labels = km.fit_predict(X)\n\n        rows.append(\n            {\n                \"k\": k,\n                \"silhouette\": silhouette_score(X, labels),\n                \"davies_bouldin\": davies_bouldin_score(X, labels),\n                \"calinski_harabasz\": calinski_harabasz_score(X, labels),\n            }\n        )\n\n    return pd.DataFrame(rows)\n\n\nresults = kmeans_metrics(X_, k_min=2, k_max=14)\nprint(results)\n\n# Highlight the “best” K by each metric\nbest_k_sil = results.loc[results[\"silhouette\"].idxmax(), \"k\"]\nbest_k_db  = results.loc[results[\"davies_bouldin\"].idxmin(), \"k\"]\nbest_k_ch  = results.loc[results[\"calinski_harabasz\"].idxmax(), \"k\"]\n\nprint(\n    f\"\\nBest-by-metric suggestions:\\n\"\n    f\"  • Silhouette         ➜ K = {best_k_sil}\\n\"\n    f\"  • Davies-Bouldin (↓) ➜ K = {best_k_db}\\n\"\n    f\"  • Calinski-Harabasz  ➜ K = {best_k_ch}\"\n)\n\n     k  silhouette  davies_bouldin  calinski_harabasz\n0    2    0.704250        0.268098        2791.480831\n1    3    0.490419        0.658522        3178.371016\n2    4    0.451653        0.866476        3456.097292\n3    5    0.462684        0.873393        3445.233408\n4    6    0.487050        0.807228        3559.362944\n5    7    0.480572        0.741292        3690.915275\n6    8    0.544377        0.714926        4381.382113\n7    9    0.488916        0.693168        4233.595603\n8   10    0.495985        0.804549        4286.893976\n9   11    0.498417        0.784347        4326.286214\n10  12    0.488947        0.784427        4235.420426\n11  13    0.465013        0.815752        3995.008086\n12  14    0.466522        0.820545        4087.451788\n\nBest-by-metric suggestions:\n  • Silhouette         ➜ K = 2\n  • Davies-Bouldin (↓) ➜ K = 2\n  • Calinski-Harabasz  ➜ K = 8\n\n\nThis notebook will run k=7 in accordance with the Calinski-Harabasz metric and the elbow method. However, we will run k=2 in the following chapter.",
    "crumbs": [
      "Running K-Means",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Running k=7</span>"
    ]
  },
  {
    "objectID": "Notebooks/k7.html#run-k-means-with-k7",
    "href": "Notebooks/k7.html#run-k-means-with-k7",
    "title": "4  Running k=7",
    "section": "4.3 Run K-Means with k=7",
    "text": "4.3 Run K-Means with k=7\n\nK=7\nkmeans = KMeans(n_clusters=K, random_state=42, n_init=20)\ncluster_labels = kmeans.fit_predict(X_)\ncentroids_pca = kmeans.cluster_centers_ \ndf[\"cluster\"] = cluster_labels\n\n\ncluster_means = df.groupby(\"cluster\").mean().round(2)\ncluster_means.drop(\"Unnamed: 0\", axis=1, inplace=True)\ncluster_means\n\n\n\n\n\n\n\n\nArea_m2\nAUC\npct_prot\nelev_mean_\nelev_std_m\nn_wells\nGround Water Rights 1966-01-01\nGround Water Rights 1967-01-01\nGround Water Rights 1968-01-01\nGround Water Rights 1969-01-01\n...\nNDWI 2019-03\nNDWI 2019-04\nNDWI 2019-05\nNDWI 2019-06\nNDWI 2019-07\nNDWI 2019-08\nNDWI 2019-09\nNDWI 2019-10\nNDWI 2019-11\nNDWI 2019-12\n\n\ncluster\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n58469.02\n108.73\n8.83\n4187.01\n6.91\n0.00\n0.0\n0.0\n0.0\n0.0\n...\n0.10\n0.09\n0.08\n0.06\n0.06\n0.04\n0.04\n0.05\n0.07\n0.08\n\n\n1\n232978.72\n101.47\n50.32\n4535.56\n6.40\n0.00\n0.0\n0.0\n0.0\n0.0\n...\n0.07\n0.05\n0.03\n-0.00\n-0.02\n-0.03\n-0.02\n-0.01\n0.01\n0.02\n\n\n2\n20100.00\n102.58\n18.44\n4115.19\n5.90\n3778.00\n80.0\n80.0\n80.0\n80.0\n...\n0.06\n0.06\n0.05\n0.04\n0.03\n0.01\n0.01\n0.03\n0.04\n0.04\n\n\n3\n21100.81\n110.20\n10.93\n4226.67\n8.19\n57.13\n0.0\n0.0\n0.0\n0.0\n...\n0.07\n0.06\n0.05\n0.02\n0.01\n-0.01\n-0.02\n-0.00\n0.01\n0.02\n\n\n4\n59534.07\n94.73\n99.88\n4398.82\n6.09\n0.00\n0.0\n0.0\n0.0\n0.0\n...\n0.04\n0.03\n0.01\n-0.01\n-0.02\n-0.03\n-0.03\n-0.01\n-0.01\n0.00\n\n\n5\n74028.60\n103.83\n24.23\n4266.85\n7.55\n0.00\n0.0\n0.0\n0.0\n0.0\n...\n0.09\n0.08\n0.07\n0.05\n0.04\n0.03\n0.03\n0.04\n0.04\n0.07\n\n\n6\n43987.50\n96.70\n0.00\n4216.39\n4.37\n0.00\n0.0\n0.0\n0.0\n0.0\n...\n0.08\n0.08\n0.06\n0.04\n0.03\n0.02\n0.01\n0.04\n0.07\n0.09\n\n\n\n\n7 rows × 2289 columns\n\n\n\n\nprint(\"Cluster counts:\")\nprint(df[\"cluster\"].value_counts().sort_index())\n\nCluster counts:\ncluster\n0    523\n1    282\n2    162\n3    247\n4    819\n5    437\n6     64\nName: count, dtype: int64\n\n\nList of Cluster Abbreviations - Cluster 0: Colchane y Pica Este (CPE) - Cluster 1: Caquena y Parque Lauca Norte (CPLN) - Cluster 2: Precordillera Tamarugal (PT) - Cluster 3: Precordillera Putre (PP) - Cluster 4: Parque Lauca Sur y Reserva Las Vicuñas (PLSRV) - Cluster 5: General Lagos (GL) - Cluster 6: Ollagüe y Calama (OC)\n\ncluster_palette = {\n    \"GL\":   \"#693B11\", # Brown\n    \"CPLN\": \"#F38D14\", # Orange\n    \"PLSRV\":\"#9863D0\", # Purple\n    \"PP\":   \"#C81A00\", # Red\n    \"CPE\":  \"#3973ac\", # Blue\n    \"PT\":   \"#339933\", # Green\n    \"OC\":   \"#ff99dd\", # Pink\n}\n\ndf[\"cluster_name\"] = df[\"cluster\"].map({0: \"Colchane y Pica Este\", 1: \"Caquena y Parque Lauca Norte\", 2: \"Precordillera Tamarugal\", 3: \"Precordillera Putre\", 4: \"Parque Lauca Sur y Reserva Las Vicuñas\", 5: \"General Lagos\", 6: \"Ollagüe y Calama\"})\ndf[\"cluster_abr\"] = df[\"cluster\"].map({0: \"CPE\", 1: \"CPLN\", 2: \"PT\", 3: \"PP\", 4: \"PLSRV\", 5: \"GL\", 6: \"OC\"})\n# Save the dataset\ndf.to_csv(\"bofedales-clusters7.csv\")",
    "crumbs": [
      "Running K-Means",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Running k=7</span>"
    ]
  },
  {
    "objectID": "Notebooks/k7.html#numerical-analysis",
    "href": "Notebooks/k7.html#numerical-analysis",
    "title": "4  Running k=7",
    "section": "4.4 Numerical Analysis",
    "text": "4.4 Numerical Analysis\nOverall, the clusters seem well-seperated\n\ninertia = kmeans.inertia_\navg_sq_dist_per_sample = inertia / X_.shape[0]\n\nprint(f\"Average squared distance per point: {avg_sq_dist_per_sample:.4f}\")\n\nAverage squared distance per point: 211.5524\n\n\n\nmu = X_.mean(axis=0)\nglobal_mse = ((X_ - mu)**2).sum(axis=1).mean()\nglobal_mse\n\nnp.float64(2159.831865394071)\n\n\nGood reduction in MSE\n\n4.4.1 Silhouette Score\nA measure of well each point sits in its own cluster vs the next best one - s≈1 ⇒ the point is well matched to its own cluster and far from the next best cluster. - s≈0 ⇒ the point sits right on the boundary between two clusters. - s&lt;0 ⇒ the point would be better placed in another cluster.\n\nsil = silhouette_score(X_, kmeans.labels_)\nprint(\"Silhouette score:\", sil)\n\nSilhouette score: 0.523639040764242\n\n\n\nch = calinski_harabasz_score(X_, kmeans.labels_)\nprint(\"Calinski–Harabasz index:\", ch)\n\nCalinski–Harabasz index: 3878.7108041244383\n\n\n\ndb = davies_bouldin_score(X_, kmeans.labels_)\nprint(\"Davies–Bouldin index:\", db)\n\nDavies–Bouldin index: 0.759991197445623\n\n\n\nsil_vals = silhouette_samples(X_, cluster_labels)\ny_lower = 10\nplt.figure(figsize=(6,4))\n\nfor cid in np.unique(cluster_labels):\n    c_sil = sil_vals[cluster_labels == cid]\n    c_sil.sort()\n    y_upper = y_lower + len(c_sil)\n    plt.fill_betweenx(np.arange(y_lower, y_upper),\n                      0, c_sil, alpha=0.7)\n    plt.text(-0.05, y_lower + len(c_sil)/2, str(cid))\n    y_lower = y_upper + 10\n\nplt.xlabel(\"Silhouette coefficient\"); plt.ylabel(\"Cluster\")\nplt.axvline(sil_vals.mean(), color=\"red\", linestyle=\"--\")\n\nplt.title(\"Silhouette plot per cluster\")\nplt.show()",
    "crumbs": [
      "Running K-Means",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Running k=7</span>"
    ]
  },
  {
    "objectID": "Notebooks/k7.html#feature-importance-with-classification",
    "href": "Notebooks/k7.html#feature-importance-with-classification",
    "title": "4  Running k=7",
    "section": "4.5 Feature Importance with Classification",
    "text": "4.5 Feature Importance with Classification\n\nt = DecisionTreeClassifier(max_depth=6, random_state=42)\nfeature_cols = [col for col in df.columns if col != \"cluster_abr\" and col != \"cluster_name\"]\nt.fit(df[feature_cols], kmeans.labels_)\nfeat_imp = pd.Series(t.feature_importances_, index=feature_cols).sort_values(ascending=False)\nfeat_imp\n\nSurface Water Rights 2018-01-01    0.333920\ncluster                            0.225944\nPET 1996-11-01                     0.178228\nTemp Min 2005-05-01                0.123259\nPrecipitation 1986-10-01           0.093196\n                                     ...   \nTemp Max 1987-08-01                0.000000\nTemp Max 1987-07-01                0.000000\nTemp Max 1987-06-01                0.000000\nTemp Max 1987-05-01                0.000000\nTemp Max 2019-10-01                0.000000\nLength: 2291, dtype: float64\n\n\nBecause of the way the data is formatted, interpret the above as general groups of variables/times instead of specific dates. One specific column or date might represent an overall trend. For example, surface water rights in 2018 is not as important as the overall trend.\n\nfig, ax = plt.subplots(figsize=(18, 10), dpi=150)\ntree.plot_tree(\n        t,\n        feature_names=feature_cols,\n        class_names=[f\"C{i}\" for i in sorted(set(df[\"cluster_abr\"]))],\n        filled=True,\n        rounded=True,\n        fontsize=8,\n        ax=ax)\nplt.tight_layout()\n# gini is a purity measurement, where gini=0 means this leaf contains all bofedales belonging to the cluster",
    "crumbs": [
      "Running K-Means",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Running k=7</span>"
    ]
  },
  {
    "objectID": "Notebooks/k7.html#data-visualizations",
    "href": "Notebooks/k7.html#data-visualizations",
    "title": "4  Running k=7",
    "section": "4.6 Data Visualizations",
    "text": "4.6 Data Visualizations\n\npd.Series(df[\"cluster_abr\"]).value_counts().sort_index().plot.bar(color='skyblue')\nplt.xlabel(\"Cluster\"); plt.ylabel(\"Count\")\nplt.title(\"Number of bofedales per cluster\")\nplt.xticks(rotation=0)\nplt.show()\n\n\n\n\n\n\n\n\n\ngeometry = [Point(xy) for xy in zip(df.lon, df.lat)]\n\ngdf = gpd.GeoDataFrame(\n    df.copy(), \n    geometry=geometry,\n    crs=\"EPSG:4326\"\n)\n\ngdf_4326 = gdf.to_crs(4326)\ngdf_4326[\"size_for_plot\"] = gdf[\"Area_m2\"].apply(lambda a: (a**0.5))\n\ngdf_4326[\"lon\"] = gdf_4326.geometry.x\ngdf_4326[\"lat\"] = gdf_4326.geometry.y\n\ntab10 = plt.colormaps.get_cmap(\"tab10\")\ncluster_ids = sorted(gdf_4326[\"cluster_abr\"].unique())\n\ncluster_colors = {\n    cid: mcolors.to_hex(tab10(i % tab10.N))\n    for i, cid in enumerate(cluster_ids)\n}\n\ncenter = [gdf_4326.lat.mean(), gdf_4326.lon.mean()]\nm = folium.Map(center, zoom_start=6, tiles=\"OpenStreetMap\")\n\nsz = gdf_4326[\"size_for_plot\"]\nradii = np.interp(sz, (sz.min(), sz.max()), (2, 20))\n\nfor (_, row), radius in zip(gdf_4326.iterrows(), radii):\n    folium.CircleMarker(\n        location=(row.geometry.y, row.geometry.x),\n        radius=radius,\n        fill=True,\n        fill_opacity=0.6,\n        weight=0.3,\n        color=\"black\",\n        fill_color=cluster_colors[row.cluster_abr],\n        popup=(\n            f\"Cluster {row.cluster_abr}&lt;br&gt;\"\n            f\"Area = {row.Area_m2:,} m²\"\n        ),\n    ).add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nunit_mapping = {\n    \"Temp Min\": \"°C\", \n    \"Temp Max\": \"°C\", \n    \"Precipitation\": \"mm\", \n    \"Ground Water Rights\": \"L/s\", \n    \"Surface Water Rights\": \"L/s\", \n    \"PET\": \"mm\", \n    \"NDVI\": \"\", \n    \"NDWI\": \"\"\n}\nvars_order = list(unit_mapping.keys())\ndefault_var = vars_order[0]\n\n# Optional guide-lines\nguide_lines = [\n    # {\"value\": 0.0, \"orientation\": \"h\", \"label\": \"Zero line\", \"color\": \"grey\", \"style\": \"dash\", \"width\": 1},\n]\n\nlong_parts = []\nfor var in vars_order:\n    var_cols = [c for c in df.columns if c.startswith(f\"{var} \")]\n    if not var_cols:\n        continue\n\n    block = df[var_cols + [\"cluster_abr\"]].copy()\n    date_index = pd.to_datetime(\n        [c.replace(f\"{var} \", \"\") for c in var_cols], errors=\"raise\"\n    )\n    block.columns = date_index.tolist() + [\"cluster_abr\"]\n\n    block = block.melt(id_vars=\"cluster_abr\", var_name=\"date\", value_name=\"value\")\n    block[\"variable\"] = var\n    block[\"date\"] = pd.to_datetime(block[\"date\"], errors=\"raise\")\n\n    long_parts.append(block)\n\ntidy = pd.concat(long_parts, ignore_index=True)\n\ntidy[\"year\"] = tidy[\"date\"].dt.year\ntidy_annual = (\n    tidy\n      .groupby([\"cluster_abr\", \"variable\", \"year\"], as_index=False)\n      .mean(numeric_only=True)\n)\n\nclusters = tidy_annual[\"cluster_abr\"].unique()\n\nfig = go.Figure()\n\nfor clus in clusters:\n    sub = tidy_annual.query(\"cluster_abr == @clus and variable == @default_var\")\n    fig.add_trace(\n        go.Scatter(\n            x=sub[\"year\"],\n            y=sub[\"value\"],\n            mode=\"lines\",\n            name=clus,\n            line=dict(color=cluster_palette[clus], width=2)\n        )\n    )\n\nfor ln in guide_lines:\n    if ln[\"orientation\"].lower().startswith(\"h\"):\n        fig.add_shape(\n            type=\"line\",\n            x0=tidy_annual[\"year\"].min(), x1=tidy_annual[\"year\"].max(),\n            y0=ln[\"value\"], y1=ln[\"value\"],\n            line=dict(color=ln.get(\"color\", \"red\"),\n                      dash=ln.get(\"style\", \"dash\"),\n                      width=ln.get(\"width\", 2))\n        )\n    else:\n        fig.add_shape(\n            type=\"line\",\n            y0=tidy_annual[\"value\"].min(), y1=tidy_annual[\"value\"].max(),\n            x0=ln[\"value\"], x1=ln[\"value\"],\n            line=dict(color=ln.get(\"color\", \"red\"),\n                      dash=ln.get(\"style\", \"dash\"),\n                      width=ln.get(\"width\", 2))\n        )\n\nbuttons = []\nfor var in vars_order:\n    ys = [\n        tidy_annual.query(\"cluster_abr == @clus and variable == @var\")[\"value\"].values\n        for clus in clusters\n    ]\n    label = var + (f\" ({unit_mapping[var]})\" if unit_mapping[var] else \"\")\n    buttons.append(\n        dict(\n            label  = var,\n            method = \"update\",\n            args   = [\n                {\"y\": ys},\n                {\n                  \"title\": f\"Average {var} by Cluster\",\n                  \"yaxis\": {\"title\": {\"text\": label}}\n                }\n            ]\n        )\n    )\n\nfig.update_layout(\n    xaxis_title=\"Year\",\n    yaxis_title=f\"{default_var}\" + (f\" ({unit_mapping[default_var]})\" if unit_mapping[default_var] else \"\"),\n    updatemenus=[dict(\n        type=\"dropdown\",\n        direction=\"down\",\n        showactive=True,\n        buttons=buttons,\n        x=0.0, xanchor=\"left\",\n        y=1.15, yanchor=\"top\"\n    )],\n    legend_title_text=\"Cluster\",\n    height=500, width=900,\n    margin=dict(t=90, r=20, l=60, b=50)\n)\n\nfig\n\n\n\n\n\n\n\n\n\npalette = [\n    \"indianred\",\"lightsalmon\",\"mediumaquamarine\",\"powderblue\",\"darkslateblue\",\n    \"mediumturquoise\",\"lavender\",\"palevioletred\",\"olivedrab\",\"lightpink\",\n    \"gold\",\"mediumvioletred\",\"lightcoral\",\"tomato\",\"sandybrown\",\n]\n\ndf_condensed = (\n    df[['Area_m2', 'AUC', 'pct_prot', 'elev_mean_', 'elev_std_m',\n        'n_wells', 'cluster', 'cluster_abr']]\n      .rename(columns={\n          'pct_prot': 'Percentage Protected Land (of total bofedal)',\n          'Area_m2' : 'Area (in m²)',\n          'AUC'     : 'Area of basin',\n          'elev_mean_': 'Average Elevation (in m)',\n          'elev_std_m': 'Elevation Standard Deviation (in m)',\n          'n_wells'   : 'Number of wells (per catchment area)',\n      })\n)\n\nagg = (df_condensed\n       .groupby(\"cluster_abr\", as_index=False)\n       .mean())\n\ndef make_trace(var, colour):\n    return go.Bar(\n        x=agg[\"cluster_abr\"],\n        y=agg[var],\n        marker=dict(color=colour, line=dict(color=\"black\")),\n        name=var\n    )\n\nvars_list   = list(agg.columns[1:])\ndefault_var = vars_list[0]\n\nfig = go.Figure(make_trace(default_var, palette[0]))\n\nbuttons = []\nfor i, var in enumerate(vars_list):\n    buttons.append(\n        dict(\n            label  = var,\n            method = \"update\",\n            args   = [\n                {\"y\": [agg[var]],\n                 \"marker.color\": [palette[i % len(palette)]]},\n                {\n                  \"yaxis\": {\"title\": {\"text\": var}}\n                }\n            ]\n        )\n    )\n\nfig.update_layout(\n    title=\"\",\n    yaxis=dict(title=dict(text=default_var)),\n    xaxis=dict(\n        title=\"Cluster\",\n        type=\"category\",\n        categoryorder=\"array\",\n        categoryarray=agg[\"cluster_abr\"].tolist()\n    ),\n    updatemenus=[dict(\n        type=\"dropdown\",\n        direction=\"down\",\n        showactive=True,\n        buttons=buttons,\n        x=0.0, xanchor=\"left\",\n        y=1.15, yanchor=\"top\"\n    )],\n    margin=dict(t=60, r=20, l=60, b=50),\n    height=450, width=700,\n    showlegend=False\n)\n\n\nfig",
    "crumbs": [
      "Running K-Means",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Running k=7</span>"
    ]
  },
  {
    "objectID": "Notebooks/k2.html",
    "href": "Notebooks/k2.html",
    "title": "5  Running k=2",
    "section": "",
    "text": "5.1 Loading the data and importing packages\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score, pairwise_distances_argmin_min, silhouette_samples\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport contextily as ctx\nfrom matplotlib.lines import Line2D\nimport matplotlib.dates as mdates\nimport matplotlib.colors as mcolors\nimport matplotlib.cm as cm\nimport folium\nX_ = np.load(\"scaled-df.npy\")\ndf = pd.read_csv(\"bofedales-clean.csv\")\ndf.drop(\"Unnamed: 0\", axis=1, inplace=True)\ndf.head(1)\n\n\n\n\n\n\n\n\nArea_m2\nAUC\npct_prot\nelev_mean_\nelev_std_m\nn_wells\nGround Water Rights 1966-01-01\nGround Water Rights 1967-01-01\nGround Water Rights 1968-01-01\nGround Water Rights 1969-01-01\n...\nNDWI 2019-03\nNDWI 2019-04\nNDWI 2019-05\nNDWI 2019-06\nNDWI 2019-07\nNDWI 2019-08\nNDWI 2019-09\nNDWI 2019-10\nNDWI 2019-11\nNDWI 2019-12\n\n\n\n\n0\n6300\n86.769539\n0.0\n4162.714286\n3.953815\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.03193\n0.026136\n0.022087\n0.019181\n0.023405\n0.015355\n-0.000504\n0.004056\n0.014678\n0.010436\n\n\n\n\n1 rows × 2289 columns",
    "crumbs": [
      "Running K-Means",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Running k=2</span>"
    ]
  },
  {
    "objectID": "Notebooks/k2.html#running-k2",
    "href": "Notebooks/k2.html#running-k2",
    "title": "5  Running k=2",
    "section": "5.2 Running k=2",
    "text": "5.2 Running k=2\n\nK=2\nkmeans = KMeans(n_clusters=K, random_state=42, n_init=20)\ncluster_labels = kmeans.fit_predict(X_)\ncentroids_pca = kmeans.cluster_centers_ \n\n\ndf[\"cluster\"] = cluster_labels\n\n\n5.2.1 Measuring fit\n\ninertia = kmeans.inertia_\n\navg_sq_dist_per_sample = inertia / X_.shape[0]\n\nprint(f\"Average squared distance per point: {avg_sq_dist_per_sample:.4f}\")\n\nmu = X_.mean(axis=0)\nglobal_mse = ((X_ - mu)**2).sum(axis=1).mean()\nprint(f\"Global mean: {global_mse}\")\n\nAverage squared distance per point: 907.8377\nGlobal mean: 2159.831865394074\n\n\n\npca2 = PCA(n_components=2).fit_transform(X_)\nplt.scatter(pca2[:,0], pca2[:,1], c=cluster_labels, s=10, cmap='plasma')\nplt.xlabel('PC1'); plt.ylabel('PC2')\nplt.title('K-means clusters in PCA space')\nplt.colorbar()\nplt.show()",
    "crumbs": [
      "Running K-Means",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Running k=2</span>"
    ]
  },
  {
    "objectID": "Notebooks/k2.html#data-visualizations",
    "href": "Notebooks/k2.html#data-visualizations",
    "title": "5  Running k=2",
    "section": "5.3 Data Visualizations",
    "text": "5.3 Data Visualizations\n\npd.Series(df[\"cluster\"]).value_counts().sort_index().plot.bar(color='skyblue')\nplt.xlabel(\"Cluster\"); plt.ylabel(\"Count\")\nplt.title(\"Number of bofedales per cluster\")\nplt.xticks(rotation=0)\nplt.show()\n\n\n\n\n\n\n\n\n\ngeometry = [Point(xy) for xy in zip(df.lon, df.lat)]\n\ngdf = gpd.GeoDataFrame(\n    df.copy(), \n    geometry=geometry,\n    crs=\"EPSG:4326\"\n)\n\ngdf[\"size_for_plot\"] = gdf[\"Area_m2\"].apply(lambda a: (a**0.5))\n\nscale_factor = 0.5\ngdf[\"size_for_plot\"] *= scale_factor\n\ngdf_3857 = gdf.to_crs(epsg=3857)\n\nfig, ax = plt.subplots(figsize=(8, 10))\n\ncmap = plt.get_cmap(\"tab10\")\n\nfor i, cluster_id in enumerate(sorted(gdf_3857[\"cluster\"].unique())):\n    subset = gdf_3857[gdf_3857[\"cluster\"] == cluster_id]\n    \n    ax.scatter(\n        subset.geometry.x, \n        subset.geometry.y,\n        s=subset[\"size_for_plot\"],\n        c=[cmap(i)],\n        alpha=0.6,\n        edgecolor=\"k\",\n        linewidth=0.3,\n        label=f\"Cluster {cluster_id}\"\n    )\n\nctx.add_basemap(\n    ax,\n    source=ctx.providers.OpenStreetMap.Mapnik,\n)\n\ncluster_handles = []\nfor idx, cluster_id in enumerate(sorted(gdf_3857[\"cluster\"].unique())):\n    cluster_handles.append(\n        Line2D(\n            [], [], \n            marker=\"o\", \n            markersize=6,\n            color=cmap(idx),\n            linestyle=\"\",\n            label=f\"Cluster {cluster_id}\",\n        )\n    )\n\nlegend1 = ax.legend(handles=cluster_handles, title=\"Cluster ID\", loc=\"upper right\")\nax.add_artist(legend1)\n\nax.set_aspect('equal', adjustable='box')\n\nax.set_axis_off()\nax.set_title(\"Bofedal Clusters\", fontsize=14)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Running K-Means",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Running k=2</span>"
    ]
  },
  {
    "objectID": "Notebooks/k2.html#feature-importance",
    "href": "Notebooks/k2.html#feature-importance",
    "title": "5  Running k=2",
    "section": "5.4 Feature Importance",
    "text": "5.4 Feature Importance\n\nX_num = (\n    df \n    .select_dtypes('number')\n    .drop(columns=['cluster'], errors='ignore')\n)\n\nscaler = StandardScaler()\nX_scaled = pd.DataFrame(\n    scaler.fit_transform(X_num),\n    columns=X_num.columns, \n    index=X_num.index\n)\n\npca = PCA(n_components=6, svd_solver='full').fit(X_scaled)\n\npc1_loadings = pd.Series(\n    pca.components_[0],\n    index=X_scaled.columns,\n    name='PC1 loading'\n)\n\nprint(pc1_loadings.sort_values(key=np.abs, ascending=False).head(20))\n\nTemp Max 2004-09-01    0.024813\nTemp Max 1981-09-01    0.024785\nTemp Max 1997-09-01    0.024768\nTemp Max 1997-08-01    0.024747\nTemp Max 1982-09-01    0.024742\nTemp Max 2013-01-01    0.024735\nTemp Max 1993-01-01    0.024733\nTemp Max 2015-01-01    0.024730\nTemp Max 1979-01-01    0.024713\nTemp Max 2004-08-01    0.024706\nTemp Max 1987-01-01    0.024690\nTemp Max 2001-01-01    0.024690\nTemp Max 2005-02-01    0.024681\nTemp Max 2011-05-01    0.024677\nTemp Max 2008-01-01    0.024676\nTemp Max 1994-09-01    0.024675\nTemp Max 1992-01-01    0.024672\nTemp Max 2001-08-01    0.024669\nTemp Max 1997-04-01    0.024662\nTemp Max 2003-09-01    0.024662\nName: PC1 loading, dtype: float64\n\n\n\nfeatures = df.columns\nfamilies = {\n    \"NDVI\":  [c for c in features if c.startswith(\"NDVI\")],\n    \"NDWI\":  [c for c in features if c.startswith(\"NDWI\")],\n    \"GW_rights\": [c for c in features if c.startswith(\"Ground Water Rights\")],\n    \"SW_rights\": [c for c in features if c.startswith(\"Surface Water Rights\")],\n    \"Temperature\": [c for c in features if c.startswith((\"Temp Min\", \"Temp Max\"))],\n    \"PET\": [c for c in features if c.startswith(\"PET\")],\n    \"Precipitation\": [c for c in features if c.startswith(\"Precipitation\")],\n    \"Size\":   [\"Area_m2\", \"AUC\"],\n    \"Elevation\": [c for c in features if c.startswith((\"elev_std_m\", \"elev_mean_\"))],\n    \"Boreholes\": [c for c in features if c.startswith(\"n_wells\")],\n    \"Protected Land\": [c for c in features if c.startswith(\"pct_prot\")]\n}\n\nsummary = {k: pc1_loadings[fam].abs().sum() for k, fam in families.items()}\npd.Series(summary).sort_values(ascending=False)\n\nTemperature       23.424659\nPET               11.547013\nPrecipitation      7.095087\nSW_rights          1.571825\nGW_rights          1.120259\nNDVI               0.176790\nNDWI               0.094735\nBoreholes          0.020540\nElevation          0.008832\nProtected Land     0.008541\nSize               0.004697\ndtype: float64\n\n\n\npc1_series = pd.Series(\n    pca.transform(X_scaled)[:, 0],\n    index=X_scaled.index,\n    name=\"PC1\"\n)\n\ncorrs = (\n    X_scaled\n        .join(pc1_series) \n        .corr(method=\"pearson\")\n        .loc[\"PC1\"]\n        .drop(\"PC1\")\n        .abs()\n        .sort_values(ascending=False)\n)\n\nprint(\"\\nTop |correlations| with PC1 (pandas-only):\")\ndisplay(corrs.head(20))\n\nfamily_corrs = (\n    corrs\n      .groupby(corrs.index.str.split().str[0])\n      .sum()\n      .sort_values(ascending=False)\n)\n\nprint(\"\\nPC-1 |corr| summed by variable family:\")\ndisplay(family_corrs.head(20))\n\n\nTop |correlations| with PC1 (pandas-only):\n\n\nTemp Max 2004-09-01    0.996245\nTemp Max 1981-09-01    0.995105\nTemp Max 1997-09-01    0.994448\nTemp Max 1997-08-01    0.993573\nTemp Max 1982-09-01    0.993372\nTemp Max 2013-01-01    0.993104\nTemp Max 1993-01-01    0.993047\nTemp Max 2015-01-01    0.992907\nTemp Max 1979-01-01    0.992210\nTemp Max 2004-08-01    0.991960\nTemp Max 1987-01-01    0.991310\nTemp Max 2001-01-01    0.991286\nTemp Max 2005-02-01    0.990943\nTemp Max 2011-05-01    0.990770\nTemp Max 2008-01-01    0.990724\nTemp Max 1994-09-01    0.990708\nTemp Max 1992-01-01    0.990597\nTemp Max 2001-08-01    0.990472\nTemp Max 1997-04-01    0.990195\nTemp Max 2003-09-01    0.990191\nName: PC1, dtype: float64\n\n\n\nPC-1 |corr| summed by variable family:\n\n\nTemp             940.498754\nPET              463.611932\nPrecipitation    284.867326\nSurface           63.108687\nGround            44.978350\nNDVI               7.098089\nNDWI               3.803588\nn_wells            0.824699\nlat                0.412471\npct_prot           0.342911\nelev_mean_         0.332095\nlon                0.249555\nAUC                0.135419\nArea_m2            0.053157\nelev_std_m         0.022529\nName: PC1, dtype: float64\n\n\nRunning the algorithm with k=2 seems to create clusters that depend almost entirely on temperature, so the seperation does not seem very meaningful.",
    "crumbs": [
      "Running K-Means",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Running k=2</span>"
    ]
  },
  {
    "objectID": "Notebooks/k3.html",
    "href": "Notebooks/k3.html",
    "title": "6  Running Without Temporal Data (k=3)",
    "section": "",
    "text": "import pandas as pd\nimport re\nfrom datetime import datetime\nimport numpy as np\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\nfrom sklearn.preprocessing import StandardScaler\n\nimport matplotlib.pyplot as plt\nfrom ipywidgets import interact, widgets, Dropdown\nimport seaborn as sns\nimport plotly.graph_objects as go\n\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport contextily as ctx\nfrom matplotlib.lines import Line2D\nimport matplotlib.dates as mdates\nimport matplotlib.colors as mcolors\nimport matplotlib.cm as cm\nimport folium\n\nimport plotly.express as px\n\n\ndf = pd.read_csv(\"bofedales-clean.csv\")\ndf.drop([\"Unnamed: 0\"], axis=1, inplace=True)\ndf.head(1)\n\n\n\n\n\n\n\n\nArea_m2\nAUC\npct_prot\nelev_mean_\nelev_std_m\nn_wells\nGround Water Rights 1966-01-01\nGround Water Rights 1967-01-01\nGround Water Rights 1968-01-01\nGround Water Rights 1969-01-01\n...\nNDWI 2019-03\nNDWI 2019-04\nNDWI 2019-05\nNDWI 2019-06\nNDWI 2019-07\nNDWI 2019-08\nNDWI 2019-09\nNDWI 2019-10\nNDWI 2019-11\nNDWI 2019-12\n\n\n\n\n0\n6300\n86.769539\n0.0\n4162.714286\n3.953815\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.03193\n0.026136\n0.022087\n0.019181\n0.023405\n0.015355\n-0.000504\n0.004056\n0.014678\n0.010436\n\n\n\n\n1 rows × 2289 columns\n\n\n\n\ndf.drop([\"lat\", \"lon\"], axis=1, inplace=True)\n\n\nAGG_POLICY = {\n    # family-name      how to aggregate?  (\"latest\"  or  \"mean\")\n    \"Precipitation\":   \"mean\", \n    \"PET\":             \"mean\",\n    \"Temp Max\":        \"mean\",\n    \"Temp Min\":        \"mean\",\n    \"Surface Water Rights\": \"latest\", \n    \"Ground Water Rights\":  \"latest\",\n    \"NDWI\": \"mean\",\n    \"NDVI\": \"mean\"\n}\n\n\ndef condense_temporal(df, policy):\n    \"\"\"\n    Collapse temporal columns based on `policy` dict.\n    Returns a NEW DataFrame (original cols dropped, new cols added).\n    \"\"\"\n    df = df.copy()\n    for family, rule in policy.items():\n        pattern = re.compile(rf\"^{re.escape(family)}\\s\")\n        fam_cols = [c for c in df.columns if pattern.match(c)]\n        if not fam_cols:\n            print(f\" No columns found for family: {family}\")\n            continue\n\n        if rule == \"mean\":\n            new_col = df[fam_cols].mean(axis=1)\n            new_name = f\"{family}\"\n        elif rule == \"latest\":\n            def _parse_date(col):\n                s = col.replace(family, \"\").strip()\n                try:\n                    return datetime.strptime(s, \"%Y-%m-%d\")\n                except ValueError:\n                    return datetime.strptime(s, \"%Y-%m\")\n            latest_col = max(fam_cols, key=_parse_date)\n            new_col = df[latest_col]\n            new_name = f\"{family}\"\n        else:\n            raise ValueError(f\"Unknown rule '{rule}' for {family}\")\n\n        df[new_name] = new_col\n        df = df.drop(columns=fam_cols)\n\n    return df\n\n\ndf_condensed = condense_temporal(df, AGG_POLICY)\n\ndf_condensed = df_condensed.rename(columns={'pct_prot': 'Percentage Protected Land (of total bofedal)', \n                        'Area_m2': 'Area (m²)', \n                        'AUC': 'Area of basin', \n                        'elev_mean_': 'Average Elevation (m)',\n                        'elev_std_m': 'Elevation Standard Deviation (m)',\n                        'n_wells': 'Number of wells (per catchment area)',\n                        'Precipitation': 'Precipitation (mm)',\n                        'PET': 'PET (mm)',\n                        'Temp Max': 'Maximum Temperature (°C)',\n                        'Temp Min': 'Minimum Temperature (°C)',\n                        'Surface Water Rights': 'Surface Water Rights (L/s)',\n                        'Ground Water Rights': 'Ground Water Rights (L/s)',\n                    })\n\nfeatures_df = (\n    df_condensed\n    .select_dtypes(\"number\")\n    .drop(columns=[\"cluster\"], errors=\"ignore\")\n)\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(features_df)\n\n\ndef kmeans_diagnostics(\n        features_df,\n        k_range=range(2, 11),\n        scaler=None,\n        random_state=0\n    ):\n    \"\"\"\n    Runs KMeans for each k in k_range, computes cluster-validity metrics,\n    makes diagnostic plots, and returns a DataFrame of the results.\n    \"\"\"\n    X = features_df.select_dtypes('number').copy()\n    \n    scaler = scaler or StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    inertias, sils, dbs, chs = [], [], [], []\n\n    for k in k_range:\n        km = KMeans(n_clusters=k, random_state=random_state, n_init=\"auto\")\n        labels = km.fit_predict(X_scaled)\n\n        inertias.append(km.inertia_)\n        sils.append(silhouette_score(X_scaled, labels))\n        dbs.append(davies_bouldin_score(X_scaled, labels))\n        chs.append(calinski_harabasz_score(X_scaled, labels))\n\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n    \n    axes[0].plot(k_range, inertias, \"o-\")\n    axes[0].set(\n        xlabel=\"k (clusters)\",\n        ylabel=\"Inertia (WCSS)\",\n        title=\"Elbow method\"\n    )\n    \n    axes[1].plot(k_range, sils, \"o-\", label=\"Silhouette ↑\")\n    axes[1].plot(k_range, dbs,  \"o-\", label=\"Davies–Bouldin ↓\")\n    axes[1].plot(k_range, chs,  \"o-\", label=\"Calinski–Harabasz ↑\")\n    axes[1].set(\n        xlabel=\"k (clusters)\",\n        title=\"Cluster-validity indices\"\n    )\n    axes[1].legend()\n    plt.tight_layout()\n    plt.show()\n\n    metrics = pd.DataFrame({\n        \"k\": list(k_range),\n        \"inertia\": inertias,\n        \"silhouette\": sils,\n        \"davies_bouldin\": dbs,\n        \"calinski_harabasz\": chs\n    })\n    return metrics\n\nmetrics = kmeans_diagnostics(\n    features_df,\n    k_range=range(2, 11), \n    random_state=42\n)\n\ndisplay(metrics.sort_values(\"silhouette\", ascending=False))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nk\ninertia\nsilhouette\ndavies_bouldin\ncalinski_harabasz\n\n\n\n\n0\n2\n24487.019836\n0.567626\n0.561799\n1136.279464\n\n\n1\n3\n18113.360620\n0.323346\n1.113283\n1213.057466\n\n\n2\n4\n15573.777886\n0.312550\n1.309326\n1077.738599\n\n\n3\n5\n14130.147582\n0.283505\n1.341564\n955.114949\n\n\n7\n9\n9894.428930\n0.274834\n1.247715\n816.033288\n\n\n6\n8\n10458.801158\n0.270735\n1.232154\n863.161538\n\n\n5\n7\n12153.917345\n0.261101\n1.425589\n808.174314\n\n\n4\n6\n13983.328016\n0.256730\n1.605868\n777.117932\n\n\n8\n10\n9747.609364\n0.250140\n1.413670\n740.220904\n\n\n\n\n\n\n\nDue to the above metrics, we use k=3 for the algorithm\n\nk=3\nkmeans = KMeans(n_clusters=k, random_state=0, n_init=\"auto\").fit(X_scaled)\ndf_condensed[\"cluster\"] = kmeans.labels_\ndf_condensed.head(3)\n\n\n\n\n\n\n\n\nArea (m²)\nArea of basin\nPercentage Protected Land (of total bofedal)\nAverage Elevation (m)\nElevation Standard Deviation (m)\nNumber of wells (per catchment area)\nPrecipitation (mm)\nPET (mm)\nMaximum Temperature (°C)\nMinimum Temperature (°C)\nSurface Water Rights (L/s)\nGround Water Rights (L/s)\nNDWI\nNDVI\ncluster\n\n\n\n\n0\n6300\n86.769539\n0.0\n4162.714286\n3.953815\n0.0\n15.011753\n76.43679\n9.74673\n-5.034592\n1715.5\n2424.3\n-0.003601\n0.218041\n1\n\n\n1\n5400\n83.176353\n0.0\n4073.500000\n12.406316\n0.0\n15.011753\n76.43679\n9.74673\n-5.034592\n1715.5\n2424.3\n-0.046695\n0.205608\n1\n\n\n2\n6300\n103.719438\n0.0\n4278.571429\n6.161102\n0.0\n15.011753\n76.43679\n9.74673\n-5.034592\n1715.5\n2424.3\n0.035979\n0.180369\n1\n\n\n\n\n\n\n\n\nsns.pairplot(\n    df_condensed.assign(cluster=kmeans.labels_),\n    vars=[\"Maximum Temperature (°C)\", \"Precipitation (mm)\", \"PET (mm)\"],\n    hue=\"cluster\", palette=\"Set2\"\n); plt.show()\n\ncentroids = pd.DataFrame(kmeans.cluster_centers_, columns=features_df.columns)\nprint(\"\\nCentroid means (k = 3):\")\ndisplay(centroids.T)\n\n\n\n\n\n\n\n\n\nCentroid means (k = 3):\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\nArea (m²)\n-0.002319\n0.013485\n-0.094332\n\n\nArea of basin\n-0.351732\n0.279181\n0.028752\n\n\nPercentage Protected Land (of total bofedal)\n0.931268\n-0.680084\n-0.555803\n\n\nAverage Elevation (m)\n0.495675\n-0.304058\n-0.766003\n\n\nElevation Standard Deviation (m)\n-0.110698\n0.100587\n-0.094223\n\n\nNumber of wells (per catchment area)\n-0.266041\n-0.254487\n3.801576\n\n\nPrecipitation (mm)\n0.786080\n-0.356371\n-2.236167\n\n\nPET (mm)\n-0.371031\n-0.126565\n3.448225\n\n\nMaximum Temperature (°C)\n-0.461297\n-0.032838\n3.276376\n\n\nMinimum Temperature (°C)\n-0.807797\n0.357518\n2.368550\n\n\nSurface Water Rights (L/s)\n1.095665\n-0.810426\n-0.570421\n\n\nGround Water Rights (L/s)\n-0.257634\n-0.263003\n3.815851\n\n\nNDWI\n-0.341034\n0.285102\n-0.089108\n\n\nNDVI\n-0.283815\n0.231884\n-0.030463\n\n\n\n\n\n\n\n\n6.0.0.1 Results\n\nCluster 0: Cool & wet bofedales\nCluster 1: Moderate sites balancing both factors\nCluster 2: Hot & extremely dry outliers\n\n\ndf_condensed[\"lat\"] = pd.read_csv(\"bofedales-clean.csv\")[\"lat\"]\ndf_condensed[\"lon\"] = pd.read_csv(\"bofedales-clean.csv\")[\"lon\"]\ndf = df_condensed\n\n\ngeometry = [Point(xy) for xy in zip(df.lon, df.lat)]\n\ngdf = gpd.GeoDataFrame(\n    df.copy(), \n    geometry=geometry,\n    crs=\"EPSG:4326\"\n)\n\ngdf[\"size_for_plot\"] = gdf[\"Area (m²)\"].apply(lambda a: (a**0.5))\n\nscale_factor = 0.5\ngdf[\"size_for_plot\"] *= scale_factor\n\ngdf_3857 = gdf.to_crs(epsg=3857)\nfig, ax = plt.subplots(figsize=(8, 10))\n\ncmap = plt.get_cmap(\"tab10\")\n\nfor i, cluster_id in enumerate(sorted(gdf_3857[\"cluster\"].unique())):\n    subset = gdf_3857[gdf_3857[\"cluster\"] == cluster_id]\n    \n    ax.scatter(\n        subset.geometry.x,\n        subset.geometry.y,\n        s=subset[\"size_for_plot\"],\n        c=[cmap(i)],\n        alpha=0.6,\n        edgecolor=\"k\",\n        linewidth=0.3,\n        label=f\"Cluster {cluster_id}\"\n    )\n\nctx.add_basemap(\n    ax,\n    source=ctx.providers.OpenStreetMap.Mapnik,\n)\n\ncluster_handles = []\nfor idx, cluster_id in enumerate(sorted(gdf_3857[\"cluster\"].unique())):\n    cluster_handles.append(\n        Line2D(\n            [], [], \n            marker=\"o\", \n            markersize=6, \n            color=cmap(idx),\n            linestyle=\"\",\n            label=f\"Cluster {cluster_id}\",\n        )\n    )\n\nlegend1 = ax.legend(handles=cluster_handles, title=\"Cluster ID\", loc=\"upper right\")\nax.add_artist(legend1)\n\nax.set_aspect('equal', adjustable='box')\n\nax.set_axis_off()\nax.set_title(\"Bofedal Clusters\", fontsize=14)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\npalette = [\n    \"indianred\",\"lightsalmon\",\"mediumaquamarine\",\"powderblue\",\"darkslateblue\",\n    \"mediumturquoise\",\"lavender\",\"palevioletred\",\"olivedrab\",\"lightpink\",\n    \"gold\",\"mediumvioletred\",\"lightcoral\",\"tomato\",\"sandybrown\",\n    \"darkseagreen\",\"lemonchiffon\",\"darksalmon\",\"darkred\",\"firebrick\",\n    \"oldlace\",\"royalblue\",\"mediumpurple\",\"plum\"\n]\n\nagg = (df_condensed\n       .groupby(\"cluster\")\n       .mean()\n       .reset_index())\n\n\ndef make_trace(var, colour):\n    return go.Bar(\n        x=agg[\"cluster\"].astype(str),\n        y=agg[var],\n        marker=dict(color=colour, line=dict(color=\"black\")),\n        name=var\n    )\n\nvars_list = list(agg.columns[1:])\ndefault_var = vars_list[0]\n\nfig = go.Figure(make_trace(default_var, palette[0]))\n\nbuttons = []\nfor i, var in enumerate(vars_list):\n    buttons.append(\n        dict(\n            label  = var,\n            method = \"update\",\n            args   = [\n                {\"y\": [agg[var]],\n                 \"marker.color\": [palette[i % len(palette)]]},\n                {\n                  \"title\": f\"Average {var} per cluster\",\n                  \"yaxis\": {\"title\": {\"text\": var}}\n                }\n            ]\n        )\n    )\n\nfig.update_layout(\n    yaxis_title=default_var,\n    xaxis=dict(title=\"Cluster\", type=\"category\"),\n    updatemenus=[dict(\n        type=\"dropdown\",\n        direction=\"down\",\n        showactive=True,\n        buttons=buttons,\n        x=0.0, xanchor=\"left\",\n        y=1.15, yanchor=\"top\"\n    )],\n    margin=dict(t=90, r=20, l=60, b=50),\n    height=450, width=700,\n    showlegend=False\n)\n\nfig",
    "crumbs": [
      "Running K-Means",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Running Without Temporal Data (k=3)</span>"
    ]
  }
]