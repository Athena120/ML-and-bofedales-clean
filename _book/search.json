[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "K-Means Clustering & Bofedales",
    "section": "",
    "text": "Introduction",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#what-are-bofedales",
    "href": "index.html#what-are-bofedales",
    "title": "K-Means Clustering & Bofedales",
    "section": "What are bofedales?",
    "text": "What are bofedales?\nBofedales are unique high-Andean wetland ecosystems characterized by peat formation. Located over 3,000m above sea level, they originate due to permanent water flows that foster the development of cushion plants. Cushion-forming Juncaceae are “nurse plants” for plants consumed by wild and domesticated camelids. Bofedales are essential for water management, have high cultural value for indigenous communities (especially herders and farmers), and play a key role in the global climate systems. They can be found in Peru, Bolivia, and northern Chile, the region focused on by this study.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#what-is-a-k-means-clustering-algorithm",
    "href": "index.html#what-is-a-k-means-clustering-algorithm",
    "title": "K-Means Clustering & Bofedales",
    "section": "What is a k-means clustering algorithm?",
    "text": "What is a k-means clustering algorithm?\nK-means clustering is a type of unsupervised machine learning model. If all of the n features of the dataset were graphed in a n-dimensional space, k-means attempts to create a set of centers and minimize the sum of squared distances between any point and its center. Since this problem is NP-hard, k-means is a heuristic algorithm that converges as a local optimum.\n\nThis project is part of the Núcleo Milenio Andespeat, a research center funded by ANID and based at the Universidad de Tarapacá in Arica, Chile. It was developed by Estella Newton and advised by Manuel Prieto, Charlie Hoffs, and Francisco Mayol.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "Notebooks/k7.html",
    "href": "Notebooks/k7.html",
    "title": "4  Running k=7",
    "section": "",
    "text": "4.1 Import Statements\nSince the merge between the datasets created a lot of columns (over 2,000), it’s necesary to reduce the dimensions of the dataset with PCA before running the k-means algorithm.\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score, pairwise_distances_argmin_min, silhouette_samples\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\n\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport contextily as ctx\nimport folium\n\nfrom matplotlib.lines import Line2D\nimport matplotlib.dates as mdates\nimport matplotlib.lines as mlines\nimport matplotlib.colors as mcolors\nimport matplotlib.cm as cm\n\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interactive\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nX_ = np.load(\"scaled-df.npy\")\ndf = pd.read_csv(\"bofedales-clean.csv\")\nThe K-Means algorithm will run on the dataset in PC space.",
    "crumbs": [
      "Running K-Means",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Running k=7</span>"
    ]
  },
  {
    "objectID": "Notebooks/k7.html#find-the-optional-amount-of-groups-for-k-means",
    "href": "Notebooks/k7.html#find-the-optional-amount-of-groups-for-k-means",
    "title": "4  Running k=7",
    "section": "4.2 Find the Optional Amount of Groups for K-Means",
    "text": "4.2 Find the Optional Amount of Groups for K-Means\n\ninertias = []\nK_candidates = list(range(2, 15))\n\nfor k in K_candidates:\n    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n    km.fit(X_)\n    inertias.append(km.inertia_)\n\nplt.figure(figsize=(6, 4))\nplt.plot(K_candidates, inertias, marker='o', lw=2)\nplt.xlabel(\"Number of clusters K\")\nplt.ylabel(\"KMeans inertia (sum of squared distances)\")\nplt.title(\"Elbow Method on 6-PC data\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFrom the graph, it looks like around 5-7 is the optional k value. However, k=2 is also an optimal value.\n\ndef kmeans_metrics(X, k_min=2, k_max=15, random_state=42):\n    rows = []\n    for k in range(k_min, k_max + 1):\n        km = KMeans(n_clusters=k, random_state=random_state, n_init=\"auto\")\n        labels = km.fit_predict(X)\n\n        rows.append(\n            {\n                \"k\": k,\n                \"silhouette\": silhouette_score(X, labels),\n                \"davies_bouldin\": davies_bouldin_score(X, labels),\n                \"calinski_harabasz\": calinski_harabasz_score(X, labels),\n            }\n        )\n\n    return pd.DataFrame(rows)\n\n\nresults = kmeans_metrics(X_, k_min=2, k_max=14)\nprint(results)\n\n# Highlight the “best” K by each metric\nbest_k_sil = results.loc[results[\"silhouette\"].idxmax(), \"k\"]\nbest_k_db  = results.loc[results[\"davies_bouldin\"].idxmin(), \"k\"]\nbest_k_ch  = results.loc[results[\"calinski_harabasz\"].idxmax(), \"k\"]\n\nprint(\n    f\"\\nBest-by-metric suggestions:\\n\"\n    f\"  • Silhouette         ➜ K = {best_k_sil}\\n\"\n    f\"  • Davies-Bouldin (↓) ➜ K = {best_k_db}\\n\"\n    f\"  • Calinski-Harabasz  ➜ K = {best_k_ch}\"\n)\n\n     k  silhouette  davies_bouldin  calinski_harabasz\n0    2    0.704250        0.268098        2791.480831\n1    3    0.490419        0.658522        3178.371016\n2    4    0.451653        0.866476        3456.097292\n3    5    0.462684        0.873393        3445.233408\n4    6    0.487050        0.807228        3559.362944\n5    7    0.480572        0.741292        3690.915274\n6    8    0.544377        0.714926        4381.382113\n7    9    0.488916        0.693168        4233.595603\n8   10    0.495985        0.804549        4286.893976\n9   11    0.498417        0.784347        4326.286214\n10  12    0.488947        0.784427        4235.420425\n11  13    0.465013        0.815752        3995.008086\n12  14    0.466522        0.820545        4087.451788\n\nBest-by-metric suggestions:\n  • Silhouette         ➜ K = 2\n  • Davies-Bouldin (↓) ➜ K = 2\n  • Calinski-Harabasz  ➜ K = 8\n\n\nThis notebook will run k=7 in accordance with the Calinski-Harabasz metric and the elbow method. However, we will run k=2 in the following chapter.",
    "crumbs": [
      "Running K-Means",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Running k=7</span>"
    ]
  },
  {
    "objectID": "Notebooks/k7.html#run-k-means-with-k7",
    "href": "Notebooks/k7.html#run-k-means-with-k7",
    "title": "4  Running k=7",
    "section": "4.3 Run K-Means with k=7",
    "text": "4.3 Run K-Means with k=7\n\nK=7\nkmeans = KMeans(n_clusters=K, random_state=42, n_init=20)\ncluster_labels = kmeans.fit_predict(X_)\ncentroids_pca = kmeans.cluster_centers_ \ndf[\"cluster\"] = cluster_labels\n\n\ncluster_means = df.groupby(\"cluster\").mean().round(2)\ncluster_means.drop(\"Unnamed: 0\", axis=1, inplace=True)\ncluster_means\n\n\n\n\n\n\n\n\nArea_m2\nAUC\npct_prot\nelev_mean_\nelev_std_m\nn_wells\nGround Water Rights 1966-01-01\nGround Water Rights 1967-01-01\nGround Water Rights 1968-01-01\nGround Water Rights 1969-01-01\n...\nNDWI 2019-03\nNDWI 2019-04\nNDWI 2019-05\nNDWI 2019-06\nNDWI 2019-07\nNDWI 2019-08\nNDWI 2019-09\nNDWI 2019-10\nNDWI 2019-11\nNDWI 2019-12\n\n\ncluster\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n58469.02\n108.73\n8.83\n4187.01\n6.91\n0.00\n0.0\n0.0\n0.0\n0.0\n...\n0.10\n0.09\n0.08\n0.06\n0.06\n0.04\n0.04\n0.05\n0.07\n0.08\n\n\n1\n232978.72\n101.47\n50.32\n4535.56\n6.40\n0.00\n0.0\n0.0\n0.0\n0.0\n...\n0.07\n0.05\n0.03\n-0.00\n-0.02\n-0.03\n-0.02\n-0.01\n0.01\n0.02\n\n\n2\n20100.00\n102.58\n18.44\n4115.19\n5.90\n3778.00\n80.0\n80.0\n80.0\n80.0\n...\n0.06\n0.06\n0.05\n0.04\n0.03\n0.01\n0.01\n0.03\n0.04\n0.04\n\n\n3\n21100.81\n110.20\n10.93\n4226.67\n8.19\n57.13\n0.0\n0.0\n0.0\n0.0\n...\n0.07\n0.06\n0.05\n0.02\n0.01\n-0.01\n-0.02\n-0.00\n0.01\n0.02\n\n\n4\n59534.07\n94.73\n99.88\n4398.82\n6.09\n0.00\n0.0\n0.0\n0.0\n0.0\n...\n0.04\n0.03\n0.01\n-0.01\n-0.02\n-0.03\n-0.03\n-0.01\n-0.01\n0.00\n\n\n5\n74028.60\n103.83\n24.23\n4266.85\n7.55\n0.00\n0.0\n0.0\n0.0\n0.0\n...\n0.09\n0.08\n0.07\n0.05\n0.04\n0.03\n0.03\n0.04\n0.04\n0.07\n\n\n6\n43987.50\n96.70\n0.00\n4216.39\n4.37\n0.00\n0.0\n0.0\n0.0\n0.0\n...\n0.08\n0.08\n0.06\n0.04\n0.03\n0.02\n0.01\n0.04\n0.07\n0.09\n\n\n\n\n7 rows × 2289 columns\n\n\n\n\nprint(\"Cluster counts:\")\nprint(df[\"cluster\"].value_counts().sort_index())\n\nCluster counts:\ncluster\n0    523\n1    282\n2    162\n3    247\n4    819\n5    437\n6     64\nName: count, dtype: int64\n\n\nList of Cluster Abbreviations - Cluster 0: Colchane y Pica Este (CPE) - Cluster 1: Caquena y Parque Lauca Norte (CPLN) - Cluster 2: Precordillera Tamarugal (PT) - Cluster 3: Precordillera Putre (PP) - Cluster 4: Parque Lauca Sur y Reserva Las Vicuñas (PLSRV) - Cluster 5: General Lagos (GL) - Cluster 6: Ollagüe y Calama (OC)\n\ncluster_palette = {\n    \"GL\":   \"#693B11\", # Brown\n    \"CPLN\": \"#F38D14\", # Orange\n    \"PLSRV\":\"#9863D0\", # Purple\n    \"PP\":   \"#C81A00\", # Red\n    \"CPE\":  \"#3973ac\", # Blue\n    \"PT\":   \"#339933\", # Green\n    \"OC\":   \"#ff99dd\", # Pink\n}\n\ndf[\"cluster_name\"] = df[\"cluster\"].map({0: \"Colchane y Pica Este\", 1: \"Caquena y Parque Lauca Norte\", 2: \"Precordillera Tamarugal\", 3: \"Precordillera Putre\", 4: \"Parque Lauca Sur y Reserva Las Vicuñas\", 5: \"General Lagos\", 6: \"Ollagüe y Calama\"})\ndf[\"cluster_abr\"] = df[\"cluster\"].map({0: \"CPE\", 1: \"CPLN\", 2: \"PT\", 3: \"PP\", 4: \"PLSRV\", 5: \"GL\", 6: \"OC\"})\n# Save the dataset\ndf.to_csv(\"bofedales-clusters7.csv\")",
    "crumbs": [
      "Running K-Means",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Running k=7</span>"
    ]
  },
  {
    "objectID": "Notebooks/k7.html#numerical-analysis",
    "href": "Notebooks/k7.html#numerical-analysis",
    "title": "4  Running k=7",
    "section": "4.4 Numerical Analysis",
    "text": "4.4 Numerical Analysis\nOverall, the clusters seem well-seperated\n\ninertia = kmeans.inertia_\navg_sq_dist_per_sample = inertia / X_.shape[0]\n\nprint(f\"Average squared distance per point: {avg_sq_dist_per_sample:.4f}\")\n\nAverage squared distance per point: 211.5524\n\n\n\nmu = X_.mean(axis=0)\nglobal_mse = ((X_ - mu)**2).sum(axis=1).mean()\nglobal_mse\n\nnp.float64(2159.8318653940732)\n\n\nGood reduction in MSE\n\n4.4.1 Silhouette Score\nA measure of well each point sits in its own cluster vs the next best one - s≈1 ⇒ the point is well matched to its own cluster and far from the next best cluster. - s≈0 ⇒ the point sits right on the boundary between two clusters. - s&lt;0 ⇒ the point would be better placed in another cluster.\n\nsil = silhouette_score(X_, kmeans.labels_)\nprint(\"Silhouette score:\", sil)\n\nSilhouette score: 0.5236390407534037\n\n\n\nch = calinski_harabasz_score(X_, kmeans.labels_)\nprint(\"Calinski–Harabasz index:\", ch)\n\nCalinski–Harabasz index: 3878.7108039937557\n\n\n\ndb = davies_bouldin_score(X_, kmeans.labels_)\nprint(\"Davies–Bouldin index:\", db)\n\nDavies–Bouldin index: 0.759991197469673\n\n\n\nsil_vals = silhouette_samples(X_, cluster_labels)\ny_lower = 10\nplt.figure(figsize=(6,4))\n\nfor cid in np.unique(cluster_labels):\n    c_sil = sil_vals[cluster_labels == cid]\n    c_sil.sort()\n    y_upper = y_lower + len(c_sil)\n    plt.fill_betweenx(np.arange(y_lower, y_upper),\n                      0, c_sil, alpha=0.7)\n    plt.text(-0.05, y_lower + len(c_sil)/2, str(cid))\n    y_lower = y_upper + 10\n\nplt.xlabel(\"Silhouette coefficient\"); plt.ylabel(\"Cluster\")\nplt.axvline(sil_vals.mean(), color=\"red\", linestyle=\"--\")\n\nplt.title(\"Silhouette plot per cluster\")\nplt.show()",
    "crumbs": [
      "Running K-Means",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Running k=7</span>"
    ]
  },
  {
    "objectID": "Notebooks/k7.html#feature-importance-with-classification",
    "href": "Notebooks/k7.html#feature-importance-with-classification",
    "title": "4  Running k=7",
    "section": "4.5 Feature Importance with Classification",
    "text": "4.5 Feature Importance with Classification\n\nt = DecisionTreeClassifier(max_depth=6, random_state=42)\nfeature_cols = [col for col in df.columns if col != \"cluster_abr\" and col != \"cluster_name\"]\nt.fit(df[feature_cols], kmeans.labels_)\nfeat_imp = pd.Series(t.feature_importances_, index=feature_cols).sort_values(ascending=False)\nfeat_imp\n\nSurface Water Rights 2018-01-01    0.333920\ncluster                            0.225944\nPET 1996-11-01                     0.178228\nTemp Min 2005-05-01                0.123259\nPrecipitation 1986-10-01           0.093196\n                                     ...   \nTemp Max 1987-08-01                0.000000\nTemp Max 1987-07-01                0.000000\nTemp Max 1987-06-01                0.000000\nTemp Max 1987-05-01                0.000000\nTemp Max 2019-10-01                0.000000\nLength: 2291, dtype: float64\n\n\nBecause of the way the data is formatted, interpret the above as general groups of variables/times instead of specific dates. One specific column or date might represent an overall trend. For example, surface water rights in 2018 is not as important as the overall trend.\n\nfig, ax = plt.subplots(figsize=(18, 10), dpi=150)\ntree.plot_tree(\n        t,\n        feature_names=feature_cols,\n        class_names=[f\"C{i}\" for i in sorted(set(df[\"cluster_abr\"]))],\n        filled=True,\n        rounded=True,\n        fontsize=8,\n        ax=ax)\nplt.tight_layout()\n# gini is a purity measurement, where gini=0 means this leaf contains all bofedales belonging to the cluster",
    "crumbs": [
      "Running K-Means",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Running k=7</span>"
    ]
  },
  {
    "objectID": "Notebooks/k7.html#data-visualizations",
    "href": "Notebooks/k7.html#data-visualizations",
    "title": "4  Running k=7",
    "section": "4.6 Data Visualizations",
    "text": "4.6 Data Visualizations\n\npd.Series(df[\"cluster_abr\"]).value_counts().sort_index().plot.bar(color='skyblue')\nplt.xlabel(\"Cluster\"); plt.ylabel(\"Count\")\nplt.title(\"Number of bofedales per cluster\")\nplt.xticks(rotation=0)\nplt.show()\n\n\n\n\n\n\n\n\n\ngeometry = [Point(xy) for xy in zip(df.lon, df.lat)]\n\ngdf = gpd.GeoDataFrame(\n    df.copy(), \n    geometry=geometry,\n    crs=\"EPSG:4326\"\n)\n\ngdf_4326 = gdf.to_crs(4326)\ngdf_4326[\"size_for_plot\"] = gdf[\"Area_m2\"].apply(lambda a: (a**0.5))\n\ngdf_4326[\"lon\"] = gdf_4326.geometry.x\ngdf_4326[\"lat\"] = gdf_4326.geometry.y\n\ntab10 = plt.colormaps.get_cmap(\"tab10\")\ncluster_ids = sorted(gdf_4326[\"cluster_abr\"].unique())\n\ncluster_colors = {\n    cid: mcolors.to_hex(tab10(i % tab10.N))\n    for i, cid in enumerate(cluster_ids)\n}\n\ncenter = [gdf_4326.lat.mean(), gdf_4326.lon.mean()]\nm = folium.Map(center, zoom_start=6, tiles=\"OpenStreetMap\")\n\nsz = gdf_4326[\"size_for_plot\"]\nradii = np.interp(sz, (sz.min(), sz.max()), (2, 20))\n\nfor (_, row), radius in zip(gdf_4326.iterrows(), radii):\n    folium.CircleMarker(\n        location=(row.geometry.y, row.geometry.x),\n        radius=radius,\n        fill=True,\n        fill_opacity=0.6,\n        weight=0.3,\n        color=\"black\",\n        fill_color=cluster_colors[row.cluster_abr],\n        popup=(\n            f\"Cluster {row.cluster_abr}&lt;br&gt;\"\n            f\"Area = {row.Area_m2:,} m²\"\n        ),\n    ).add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nunit_mapping = {\n    \"Temp Min\": \"°C\", \n    \"Temp Max\": \"°C\", \n    \"Precipitation\": \"mm\", \n    \"Ground Water Rights\": \"L/s\", \n    \"Surface Water Rights\": \"L/s\", \n    \"PET\": \"mm\", \n    \"NDVI\": \"\", \n    \"NDWI\": \"\"\n}\nvars_order = list(unit_mapping.keys())\ndefault_var = vars_order[0]\n\n# Optional guide-lines\nguide_lines = [\n    # {\"value\": 0.0, \"orientation\": \"h\", \"label\": \"Zero line\", \"color\": \"grey\", \"style\": \"dash\", \"width\": 1},\n]\n\nlong_parts = []\nfor var in vars_order:\n    var_cols = [c for c in df.columns if c.startswith(f\"{var} \")]\n    if not var_cols:\n        continue\n\n    block = df[var_cols + [\"cluster_abr\"]].copy()\n    date_index = pd.to_datetime(\n        [c.replace(f\"{var} \", \"\") for c in var_cols], errors=\"raise\"\n    )\n    block.columns = date_index.tolist() + [\"cluster_abr\"]\n\n    block = block.melt(id_vars=\"cluster_abr\", var_name=\"date\", value_name=\"value\")\n    block[\"variable\"] = var\n    block[\"date\"] = pd.to_datetime(block[\"date\"], errors=\"raise\")\n\n    long_parts.append(block)\n\ntidy = pd.concat(long_parts, ignore_index=True)\n\ntidy[\"year\"] = tidy[\"date\"].dt.year\ntidy_annual = (\n    tidy\n      .groupby([\"cluster_abr\", \"variable\", \"year\"], as_index=False)\n      .mean(numeric_only=True)\n)\n\nclusters = tidy_annual[\"cluster_abr\"].unique()\n\nfig = go.Figure()\n\nfor clus in clusters:\n    sub = tidy_annual.query(\"cluster_abr == @clus and variable == @default_var\")\n    fig.add_trace(\n        go.Scatter(\n            x=sub[\"year\"],\n            y=sub[\"value\"],\n            mode=\"lines\",\n            name=clus,\n            line=dict(color=cluster_palette[clus], width=2)\n        )\n    )\n\nfor ln in guide_lines:\n    if ln[\"orientation\"].lower().startswith(\"h\"):\n        fig.add_shape(\n            type=\"line\",\n            x0=tidy_annual[\"year\"].min(), x1=tidy_annual[\"year\"].max(),\n            y0=ln[\"value\"], y1=ln[\"value\"],\n            line=dict(color=ln.get(\"color\", \"red\"),\n                      dash=ln.get(\"style\", \"dash\"),\n                      width=ln.get(\"width\", 2))\n        )\n    else:\n        fig.add_shape(\n            type=\"line\",\n            y0=tidy_annual[\"value\"].min(), y1=tidy_annual[\"value\"].max(),\n            x0=ln[\"value\"], x1=ln[\"value\"],\n            line=dict(color=ln.get(\"color\", \"red\"),\n                      dash=ln.get(\"style\", \"dash\"),\n                      width=ln.get(\"width\", 2))\n        )\n\nbuttons = []\nfor var in vars_order:\n    ys = [\n        tidy_annual.query(\"cluster_abr == @clus and variable == @var\")[\"value\"].values\n        for clus in clusters\n    ]\n    label = var + (f\" ({unit_mapping[var]})\" if unit_mapping[var] else \"\")\n    buttons.append(\n        dict(\n            label  = var,\n            method = \"update\",\n            args   = [\n                {\"y\": ys},\n                {\n                  \"title\": f\"Average {var} by Cluster\",\n                  \"yaxis\": {\"title\": {\"text\": label}}\n                }\n            ]\n        )\n    )\n\nfig.update_layout(\n    xaxis_title=\"Year\",\n    yaxis_title=f\"{default_var}\" + (f\" ({unit_mapping[default_var]})\" if unit_mapping[default_var] else \"\"),\n    updatemenus=[dict(\n        type=\"dropdown\",\n        direction=\"down\",\n        showactive=True,\n        buttons=buttons,\n        x=0.0, xanchor=\"left\",\n        y=1.15, yanchor=\"top\"\n    )],\n    legend_title_text=\"Cluster\",\n    height=500, width=900,\n    margin=dict(t=90, r=20, l=60, b=50)\n)\n\nfig\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\npalette = [\n    \"indianred\",\"lightsalmon\",\"mediumaquamarine\",\"powderblue\",\"darkslateblue\",\n    \"mediumturquoise\",\"lavender\",\"palevioletred\",\"olivedrab\",\"lightpink\",\n    \"gold\",\"mediumvioletred\",\"lightcoral\",\"tomato\",\"sandybrown\",\n]\n\ndf_condensed = (\n    df[['Area_m2', 'AUC', 'pct_prot', 'elev_mean_', 'elev_std_m',\n        'n_wells', 'cluster', 'cluster_abr']]\n      .rename(columns={\n          'pct_prot': 'Percentage Protected Land (of total bofedal)',\n          'Area_m2' : 'Area (in m²)',\n          'AUC'     : 'Area of basin',\n          'elev_mean_': 'Average Elevation (in m)',\n          'elev_std_m': 'Elevation Standard Deviation (in m)',\n          'n_wells'   : 'Number of wells (per catchment area)',\n      })\n)\n\nagg = (df_condensed\n       .groupby(\"cluster_abr\", as_index=False)\n       .mean())\n\ndef make_trace(var, colour):\n    return go.Bar(\n        x=agg[\"cluster_abr\"],\n        y=agg[var],\n        marker=dict(color=colour, line=dict(color=\"black\")),\n        name=var\n    )\n\nvars_list   = list(agg.columns[1:])\ndefault_var = vars_list[0]\n\nfig = go.Figure(make_trace(default_var, palette[0]))\n\nbuttons = []\nfor i, var in enumerate(vars_list):\n    buttons.append(\n        dict(\n            label  = var,\n            method = \"update\",\n            args   = [\n                {\"y\": [agg[var]],\n                 \"marker.color\": [palette[i % len(palette)]]},\n                {\n                  \"yaxis\": {\"title\": {\"text\": var}}\n                }\n            ]\n        )\n    )\n\nfig.update_layout(\n    title=\"\",\n    yaxis=dict(title=dict(text=default_var)),\n    xaxis=dict(\n        title=\"Cluster\",\n        type=\"category\",\n        categoryorder=\"array\",\n        categoryarray=agg[\"cluster_abr\"].tolist()\n    ),\n    updatemenus=[dict(\n        type=\"dropdown\",\n        direction=\"down\",\n        showactive=True,\n        buttons=buttons,\n        x=0.0, xanchor=\"left\",\n        y=1.15, yanchor=\"top\"\n    )],\n    margin=dict(t=60, r=20, l=60, b=50),\n    height=450, width=700,\n    showlegend=False\n)\n\n\nfig",
    "crumbs": [
      "Running K-Means",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Running k=7</span>"
    ]
  }
]